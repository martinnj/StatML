\section*{I.1.1.1}
Given
\[
	a = \begin{pmatrix} 1\\2\\2 \end{pmatrix}
	b = \begin{pmatrix} 3\\2\\1 \end{pmatrix}
\]
Then $a^Tb = 9$

\section*{I.1.1.2}
The \textit{l2-norm} or \textit{Euclidean norm} $||a|| = \sqrt{1^2 + 2^2 + 2^2} = 3$

\section*{I.1.1.3}
The outer product
\[
	ab^T = \begin{bmatrix}
		3 & 2 & 1 \\
		6 & 4 & 2 \\
		6 & 4 & 2
	       \end{bmatrix}
\]

\section*{I.1.1.4}
The inverse matrix of $M$ is
\[
	M^{-1} = \begin{bmatrix}
		1 & 0 & 0 \\
		0 & 0.25 & 0 \\
		0 & 0 & 0.5
	\end{bmatrix}
\]

\section*{I.1.1.5}
The matrix-vector product $Ma = \begin{pmatrix}1 \\ 8 \\ 4\end{pmatrix}$

\section*{I.1.1.6}
\[
	A = ab^T = \begin{bmatrix}
		3 & 6 & 6 \\
		2 & 4 & 4 \\
		1 & 2 & 2
	\end{bmatrix}
\]

\section*{I.1.1.7}
The rank of $A = 1$, because the rows are linearly dependent. We can verify this by observing
that the third row can produce the first and second rows with a multiple, e.g. the first row (3 6 6)
is the same as the third row (1 2 2) x 3.

\section*{I.1.1.8}
As $A$ is not full rank, it is not invertible.

\section*{I.1.2.1}
The derivative of $f(w) = (wx + b)^2$ with respect to $w$ is
\[
	(wx+b)^2 = w^2 x^2 + 2wxb+b^2 = \\
	2x^2w + 2xb = \\
	2x(xw + b)
\]

\section*{I.1.2.2}
In general
\[
	\left ( \frac{f}{g} \right )' (x) = \frac{f'(x) \cdot g(x) - f(x) \cdot g'(x)}{(g(x))^2}
\]

Therefore, differentiating for w we get:
\begin{align*}
	f(x) &= 1 \\
	f'(x) &= 0 \\
	g(x) &= (wx+b)^2 \\
	g'(x) &= 2x(wx+b) \\
	\left ( \frac{f}{g} \right )' (w) &= \frac{0 \cdot (wx+b)^2 - 1 \cdot 2x(wx+b)}{((wx+b)^2)^2} \\
	&= \frac{-1 \cdot 2x(wx+b)}{(wx+b)^4} \\
	&= \frac{-2x}{(wx+b)^3}
\end{align*}

\section*{I.1.2.3}
In general
\[
	\left ( f \cdot g \right )' (x) = f'(x) \cdot g(x) + f(x) \cdot g'(x)
\]

Therefore, differentiating for x we get:
\begin{align*}
	f(x) &= x \\
	g(x) &= e^x \\
	\left ( f \cdot g \right )' (x) &= 1e^x + xe^x
\end{align*}

\pagebreak
\section*{I.2.1}
The plots with gaussian distributions for (sigma, mu) pairs (-1,1), (0,2) and (2,3) can be
seen in Figure~\ref{fig:I.2.1}. The code for generating the plots can be found in \texttt{unigauss\_run.m}, 
and the code for our gaussian distribution function can be found in \texttt{unigauss.m}.

\begin{figure}[h!]
	\includegraphics[width=0.5\textwidth]{img/unigauss}
	\caption{Gaussian distributions plotted with different values for (sigma, mu). \label{fig:I.2.1}}
\end{figure}

\section*{I.2.2}
Source code is available in \texttt{multigauss.m} and \texttt{multigauss\_run.m}.
\begin{figure}[h!]
	\includegraphics[width=0.5\textwidth]{img/multigauss}
	\caption{100 points drawn from a 2-dimensional Multivariate gaussian distribution. \label{fig:I.2.2}}
\end{figure}

\section*{I.2.3}
The l2 norm of $x$ is 
\begin{align*}
	mean  &= \begin{pmatrix}1 & 2\end{pmatrix}^T \\
	\mu   &= \begin{pmatrix}1.0006 & 1.9834\end{pmatrix}^T \\
	||x|| &= l2(mean - \mu) = 0.0366
\end{align*}
where $l2()$ is a function that calculates the \textit{Euclidean norm} or l2 norm of the vector $mean - \mu$.

Figure~\ref{fig:I.2.3} plots the points drawn along with a red circle for the calculated mean and a green circle
for $\mu$. There is a difference between the two because the mean is calculated based on the generated data drawn
from the multivariate gaussian distribution at random. If we had a number of points approaching infinite, the difference
would approach 0.

\begin{figure}[h!]
	\includegraphics[width=0.5\textwidth]{img/multigaussmeanxy}
	\caption{100 points drawn from a 2-dimensional Multivariate gaussian distribution, plotted with
	the mean of the distribution and the value of $\mu$. \label{fig:I.2.3}}
\end{figure}

\FloatBarrier
\section*{I.2.4}
The covariance matrix is full rank 2 and thus has two eigenvectors and eigenvalues. Each eigenvector represents
a principal component (or linearly uncorrelated variable), and each eigenvalue a scalar representing the variance.
Intuitively, the eigenvectors form a scaled and translated coordinate system centered at the mean of the multivariate
Gaussian distribution ($\mu$). If an eigenvalue is 0, the dimensionality is reduced by one. The larger of the two
eigenvector/value pairs represents the direction where the ellipsis is widest.

The covariance matrix we calculated can be found in Eq~\ref{eq:cov}. Figure~\ref{fig:I.2.4.1} shows a plot of the Multivariate
gaussian distribution, plotted with the mean, $\mu$ and the two eigenvectors centered in the distribution $\mu$. Figure~\ref{fig:I.2.4.1.rot}
shows a plot of the 3 rotated distributions along with the distribution rotated to match the largest eigenvector along the x-axis. The
angle needed for this was $-37.2564^o$ in our case.

\begin{align}
	\Sigma_{ML} &= \frac{1}{N} \sum_{n=1}^N (x_n - \mu_{ML}) (x_n - \mu_{ML})^T \notag \\
	&= \begin{pmatrix}
		0.3239 & 0.2093 \\
		0.2093 & 0.2080
	\end{pmatrix} \label{eq:cov}
\end{align}

\begin{figure}[h!]
	\includegraphics[width=0.5\textwidth]{img/multigausseigen}
	\caption{100 points drawn from a 2-dimensional Multivariate gaussian distribution, plotted with the
	mean of the distribution, the value of $\mu$ and the two eigenvectors centered in the distribution $\mu$. \label{fig:I.2.4.1}}
\end{figure}

\begin{figure}[h!]
	\includegraphics[width=0.5\textwidth]{img/multigaussrotate}
	\caption{100 points drawn from a 2-dimensional Multivariate gaussian distribution, rotated at 30, 60 and 90 degrees
	and lastly also aligned along the x-axis, all distributions in their own color. \label{fig:I.2.4.1.rot}}
\end{figure}

\section*{I.3}
We have $\mu$, $\overline{x}$ and the covariance $\Sigma$ given by:
\begin{align}
	\mu &= \begin{pmatrix}
		\mu_a \\
		\mu_b \\
		\mu_c
	\end{pmatrix} \\
	\overline{x} &= \begin{pmatrix}
		x_a \\
		x_b \\
		x_c
	\end{pmatrix} \\
	\Sigma &= \begin{bmatrix}
		\Sigma_{aa} & \Sigma_{ab} & \Sigma_{ac} \\
		\Sigma_{ba} & \Sigma_{bb} & \Sigma_{bc} \\
		\Sigma_{ca} & \Sigma_{cb} & \Sigma_{cc}
	\end{bmatrix}
\end{align}

We partition our $\mu$, $\overline{x}$ and $\Sigma$ as follows:
\begin{align}
	\mu &= \begin{pmatrix}
		\mu_d \\
		\mu_c
	\end{pmatrix}, \text{where } \mu_d = \begin{pmatrix}\mu_a \\ \mu_b \end{pmatrix} \\
	\overline{x} &= \begin{pmatrix}
		x_d \\
		x_c
	\end{pmatrix}, \text{where } x_d = \begin{pmatrix}x_a \\ x_b \end{pmatrix} \\
	\Sigma &= \begin{bmatrix}
		\Sigma_{aad} & \Sigma_{abd} \\
		\Sigma_{bad} & \Sigma_{bbd} \\
	\end{bmatrix} \\
	\text{where } \Sigma_{aad} &= \begin{bmatrix}
		\Sigma_{aa} & \Sigma_{ab} \\
		\Sigma_{ba} & \Sigma_{bb}
	\end{bmatrix} \\
	\text{and } \Sigma_{abd} &= \begin{bmatrix}
		\Sigma_{ac} \\ \Sigma_{bc}
	\end{bmatrix} \\
	\text{and } \Sigma_{bad} &= \begin{bmatrix}
		\Sigma_{ca} & \Sigma_{cb} 
	\end{bmatrix} \\
	\text{and } \Sigma_{bbd} &= \begin{bmatrix} \Sigma_{cc} \end{bmatrix}
\end{align}

We also define a partitioned precision matrix 
(inverse of the covariance matrix), as:
\begin{align}
	\Lambda &\equiv \Sigma^{-1} = \begin{bmatrix}
		\Lambda_{aa} & \Lambda_{ab} \\
		\Lambda_{ba} & \Lambda_{bb}		
	\end{bmatrix}
\end{align}

We wish to now discover $p(\overline{x}_d | \overline{x}_c)$, by considering
the quadratic formin the exponent of the Gaussian distribution:

\begin{align}
	-\frac{1}{2} (x - \mu)^T \Sigma^{-1}(x - \mu)
\end{align}


\section{I.4}
\subsection{I.4.1}
The result of our KNN implementation for different k-values and datasets is
shown in Table \ref{tab:knn-results}. The file that generates these results is
\texttt{kNN_run.m}, it will setup the experiment and do the classifying using
the funktion defined in \texttt{kNN.m}.
\begin{table}
\begin{tabular}{|l|l|l|}
\hline
Description          & $K$-value & Accuracy in \% \\\hline
Run on training data & 1         & $100  \%$ \\
Run on test data     & 1         & $81.5 \%$ \\
Run on training data & 3         & $86.0 \%$ \\
Run on test data     & 3         & $81.5 \%$ \\
Run on training data & 5         & $83.0 \%$ \\
Run on test data     & 5         & $68.4 \%$
\end{tabular}
\label{tab:knn-results}
\end{table}

With $K=1$ and running against the training set, the accuracy is 100\% since any
entry will be matched against itself, and only itself. We also see a general
loss of accuracy as $K$ increases, this is because the point gets matched up
against a larger and larger part of the total points, meaning it will become
more likely to be classified as the type of point there is occuring most times
in the training set.


\subsection{I.4.2}
The program run for this experiment is \texttt{fivefold_val.m}, it will use aux
functions from \texttt{kNN.m}, \texttt{shuffleSplit.m} and
\texttt{bucketJoiner.m}. We first split the training data using the
\texttt{shuffleSplit} function, it creates a cell-table that contains a
specified number of ``subsets'' that collectively is the entire training set,
but where the indices of the data is shuffled. We then assemble these parts into
2 sets, one that is 1/5 of the data, which will be used as a testing set, and
one with 4/5 of the data, which is the training set.  using five different
partitions of the data (with the above method) we used the \texttt{kNN} function
from the previous subsection to benchmark the different $K$ values with 5
``different training sets''.

During this test, a $K$ value of $5$ gets the best accuracy, around 80\%, but
when run against the actual training set, which have not been used or considered
in the above process, the accuracy drops to $68,4\%$. This is the same accuracy
we got from $K=5$ in the previous section.

\subsection{I.4.3}
The program used for this experiment is \texttt{fivefold_val_normalized.m} which
is similar to the above experiment, except it also utilizes \texttt{scale.m} to
normalize the test data.  The $(\text{mean},\text{var})$ of the training data
is: $(3.0288,7.8218)$, the test data after the normalization have the values
$(0.1545,1.0000)$. The most optimal $K$ value is still 5, but the accuracy have
increased to $71,05\%$ when using the normalized test set.
