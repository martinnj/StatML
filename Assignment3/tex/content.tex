\section*{III.1.1}
\section*{III.1.2}

\section*{III.2.1}
\textbf{Mean/variuance of the training data}\\
\begin{tabular}{|r|r|r|r|r|r|}
\hline
priorTrainMean & priorTrainVar & trainMean & trainVar & testMean & testVar \\\hline
         &$1.0e+03 *$ & $1.0e-14 *$ & & & \\
155.9604 & 1.9830 &  0.0180 & 1.0000 & -0.0782 & 0.7323 \\
204.8212 & 9.7331 & -0.0623 & 1.0000 & -0.1572 & 0.7150 \\
115.0586 & 2.1152 & -0.0305 & 1.0000 &  0.0553 & 0.7977 \\
  0.0060 & 0.0000 &  0.0151 & 1.0000 &  0.1126 & 1.9906 \\
  0.0000 & 0.0000 & -0.1268 & 1.0000 &  0.0712 & 1.6662 \\
  0.0032 & 0.0000 & -0.0344 & 1.0000 &  0.0865 & 2.1370 \\
  0.0033 & 0.0000 &  0.0238 & 1.0000 &  0.1151 & 1.9225 \\
  0.0096 & 0.0000 & -0.0019 & 1.0000 &  0.0866 & 2.1379 \\
  0.0277 & 0.0000 &  0.1217 & 1.0000 &  0.2477 & 1.7721 \\
  0.2624 & 0.0000 & -0.0762 & 1.0000 &  0.2439 & 1.8292 \\
  0.0147 & 0.0000 &  0.1304 & 1.0000 &  0.2284 & 1.7175 \\
  0.0166 & 0.0000 & -0.0491 & 1.0000 &  0.2496 & 1.7780 \\
  0.0220 & 0.0000 &  0.0597 & 1.0000 &  0.3150 & 2.1905 \\
  0.0440 & 0.0000 & -0.0129 & 1.0000 &  0.2284 & 1.7176 \\
  0.0226 & 0.0000 &  0.0140 & 1.0000 &  0.1483 & 2.6633 \\
 22.0007 & 0.0167 & -0.2299 & 1.0000 & -0.0565 & 1.3610 \\
  0.4948 & 0.0000 &  0.1184 & 1.0000 &  0.0732 & 1.0827 \\
  0.7157 & 0.0000 &  0.3141 & 1.0000 &  0.0863 & 0.9514 \\
 -5.7637 & 0.0011 & -0.1559 & 1.0000 &  0.1540 & 1.2166 \\
  0.2148 & 0.0000 &  0.0715 & 1.0000 &  0.3091 & 1.3629 \\
  2.3658 & 0.0001 & -0.1414 & 1.0000 &  0.0870 & 1.1336 \\
  0.1997 & 0.0000 & -0.0476 & 1.0000 &  0.1677 & 1.4149 \\\hline
\end{tabular}

\section*{III.2.2}

\textit{Description of the software used:}\\
We used LibSVM for
Matlab\footnote{\url{http://www.csie.ntu.edu.tw/\~cjlin/libsvm/\#matlab}}.
Training the SVM produces a model structure, this is done like so:
\begin{verbatim}
model = svmtrain(trainX, trainY, 'kernel_function','rbf',
                 'rbf_sigma',sigmas,'boxconstraint',C);
\end{verbatim}


\begin{tabular}{|l l|}
\hline
trainX                      & Training data features.\\
trainY                      & Corresponding training data classes.\\
'kernel\_function' \& 'rbf' & Sets the kernel function to be a gaussian kernel function.\\
'rbf\_sigma' \& sigmas      & Lets us specify a $\sigma = \sqrt{1/(2\gamma)}$.\\
'boxconstraint' \& C        & Lets us specify regularization paramter.\\\hline
\end{tabular}

\noindent \textit{Our process:}\\
For our five-fold cross validation we re-used the code (\texttt{shuffleSplit}
and \texttt{bucketJoiner}) from the first assignment. So for each C and each
$\gamma$ we did a five-fold cross validation. The best C and Gamma combination
was extracted and used to train a new model that could be used for
classification.

\noindent \textit{Results:}\\
We used the following numbers for $C$ and $\gamma$: $C = (0.01, 0.1, 1.0, 10,
100, 1000, 10000)$ and $\gamma = (0.0001, 0.001, 0.01, 0.1, 1, 10, 100)$. For
the raw data we found the best $C$ and $\gamma$ to be $C=0.01,
\gamma=0.001$. For the normalized data we found $C=100, \gamma=0.1$.

\begin{tabular}{l l}
Prenormalized training data accuracy & $ 46.94\%$\\
Prenormalized test data accuracy     & $ 46.39\%$\\
Normalized training data accuracy    & $100.00\%$\\
Normalized test data accuracy        & $ 89.69\%$\\
\end{tabular}

\noindent \textit{Discussion:}\\
If you do not normalize your data, you will unwillingly give more importance to
feutures wich are widely spreed, and similarly give less importeanse to dense
data.

\section*{III.2.3.1}
For our optimum hyper-parameters the number of free support vectors are $0$, and
the number of bound support vectors are $54$. Changing $C$ will change the
number of supportvectors in the model, a lower value of $C$ will result in more
supportvectors and with more free supportvectors. This means smaller $C$ values
gives you more supportvectors to work with, but the increase in free
supportvectors also means that the machine is more prone to miss-classification.

\section*{III.2.3.2}
As the number of entries in the training data increases so does the number of
tsupportvectors in the SVM. The amount of free vectors (for our training
dataset) is roughly one third of the total number of support vectors and so does
not change with the amount of training data. The increase in the number of
supportvectors will mean the SVM takes more time to process each query
afterwards, but will also be able to do so with greater accuracy. So much like
the $C$ value, this becomes a trade-off, in this case between accuracy and
speed.
