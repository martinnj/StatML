\section*{III.1.1}
\section*{III.1.2}

\section*{III.2.1}
\textbf{Mean/variuance of the training data}\\
\begin{tabular}{|r|r|r|r|r|r|}
\hline
priorTrainMean & priorTrainVar & trainMean & trainVar & testMean & testVar \\\hline
         &$1.0e+03 *$ & $1.0e-14 *$ & & & \\
155.9604 & 1.9830 &  0.0180 & 1.0000 & -0.0782 & 0.7323 \\
204.8212 & 9.7331 & -0.0623 & 1.0000 & -0.1572 & 0.7150 \\
115.0586 & 2.1152 & -0.0305 & 1.0000 &  0.0553 & 0.7977 \\
  0.0060 & 0.0000 &  0.0151 & 1.0000 &  0.1126 & 1.9906 \\
  0.0000 & 0.0000 & -0.1268 & 1.0000 &  0.0712 & 1.6662 \\
  0.0032 & 0.0000 & -0.0344 & 1.0000 &  0.0865 & 2.1370 \\
  0.0033 & 0.0000 &  0.0238 & 1.0000 &  0.1151 & 1.9225 \\
  0.0096 & 0.0000 & -0.0019 & 1.0000 &  0.0866 & 2.1379 \\
  0.0277 & 0.0000 &  0.1217 & 1.0000 &  0.2477 & 1.7721 \\
  0.2624 & 0.0000 & -0.0762 & 1.0000 &  0.2439 & 1.8292 \\
  0.0147 & 0.0000 &  0.1304 & 1.0000 &  0.2284 & 1.7175 \\
  0.0166 & 0.0000 & -0.0491 & 1.0000 &  0.2496 & 1.7780 \\
  0.0220 & 0.0000 &  0.0597 & 1.0000 &  0.3150 & 2.1905 \\
  0.0440 & 0.0000 & -0.0129 & 1.0000 &  0.2284 & 1.7176 \\
  0.0226 & 0.0000 &  0.0140 & 1.0000 &  0.1483 & 2.6633 \\
 22.0007 & 0.0167 & -0.2299 & 1.0000 & -0.0565 & 1.3610 \\
  0.4948 & 0.0000 &  0.1184 & 1.0000 &  0.0732 & 1.0827 \\
  0.7157 & 0.0000 &  0.3141 & 1.0000 &  0.0863 & 0.9514 \\
 -5.7637 & 0.0011 & -0.1559 & 1.0000 &  0.1540 & 1.2166 \\
  0.2148 & 0.0000 &  0.0715 & 1.0000 &  0.3091 & 1.3629 \\
  2.3658 & 0.0001 & -0.1414 & 1.0000 &  0.0870 & 1.1336 \\
  0.1997 & 0.0000 & -0.0476 & 1.0000 &  0.1677 & 1.4149 \\\hline
\end{tabular}

\section*{III.2.2}

\textit{Description of the software used:}\\
We used LibSVM for
Matlab\footnote{\url{http://www.csie.ntu.edu.tw/\~cjlin/libsvm/\#matlab}}.
Training the SVM produces a model structure, this is done like so:
\begin{verbatim}
model = svmtrain(trainX, trainY, 'kernel_function','rbf',
                 'rbf_sigma',sigmas,'boxconstraint',C);
\end{verbatim}


\begin{tabular}{|l l|}
\hline
trainX                      & Training data features.\\
trainY                      & Corresponding training data classes.\\
'kernel\_function' \& 'rbf' & Sets the kernel function to be a gaussian kernel function.\\
'rbf\_sigma' \& sigmas      & Lets us specify a $\sigma = \sqrt{1/(2\gamma)}$.\\
'boxconstraint' \& C        & Lets us specify regularization paramter.\\\hline
\end{tabular}

\noindent \textit{Our process:}\\
For our five-fold cross validation we re-used the code (\texttt{shuffleSplit}
and \texttt{bucketJoiner}) from the first assignment. So for each C and each
$\gamma$ we did a five-fold cross validation. The best C and Gamma combination
was extracted and used to train a new model that could be used for
classification.

\noindent \textit{Results:}\\
We used the following numbers for $C$ and $\gamma$: $C = (0.01, 0.1, 1.0, 10,
100, 1000, 10000)$ and $\gamma = (0.0001, 0.001, 0.01, 0.1, 1, 10, 100)$. For
the raw data we found the best $C$ and $\gamma$ to be $C=0.01,
\gamma=0.001$. For the normalized data we found $C=100, \gamma=0.1$.

\begin{tabular}{l l}
Prenormalized training data accuracy &  46.94\%\\
Prenormalized test data accuracy     &  46.39\%\\
Normalized training data accuracy    & 100.00\%\\
Normalized test data accuracy        &  89.69\%\\
\end{tabular}

\noindent \textit{Discussion:}\\
If you do not normalize your data, you will unwillingly give more importance to
feutures wich are widely spreed, and similarly give less importeanse to dense
data.

\section*{III.2.3.1}
For our optimum hyper-parameters the number of free support vectors are $0$, and
the number of bound support vectors are $54$. Changing $C$ will change the
number of supportvectors in the model, a lower value of $C$ will result in more
supportvectors and with more free supportvectors. This means smaller $C$ values
gives you more supportvectors to work with, but the increase in free
supportvectors also means that the machine is more prone to miss-classification.

\section*{III.2.3.2}
